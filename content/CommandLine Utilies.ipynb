{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Unix-Command-line-Utilities:\">Unix Command line Utilities:</a></div><div class=\"lev2\"><a href=\"#wc-or-word-count\">wc or word count</a></div><div class=\"lev2\"><a href=\"#head:\">head:</a></div><div class=\"lev2\"><a href=\"#sort\">sort</a></div><div class=\"lev2\"><a href=\"#uniq\">uniq</a></div><div class=\"lev2\"><a href=\"#paste-&amp;-join\">paste &amp; join</a></div><div class=\"lev2\"><a href=\"#split\">split</a></div><div class=\"lev2\"><a href=\"#grep\">grep</a></div><div class=\"lev4\"><a href=\"#How-do-you-use-grep:\">How do you use grep:</a></div><div class=\"lev4\"><a href=\"#Regular-expressions\">Regular expressions</a></div><div class=\"lev4\"><a href=\"#Inverse-search:\">Inverse search:</a></div><div class=\"lev2\"><a href=\"#Things-to-try:\">Things to try:</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unix Command line Utilities:\n",
    "\n",
    "You might be wondering why do we need command line utilites for text processing. But the traditional unix command line tools are extremely helpfull when it comes to text processing. Most of the preprocesing and cleaning of text can be done using these tools. It would be helpfull to learn regular expressions before learning to use these tools because some of these utilites use regular experssions.\n",
    "\n",
    "\n",
    "Since there are a lot of command line utilites, we will only discuss the most used ones:\n",
    "\n",
    "1. wc \n",
    "2. head\n",
    "2. Sort\n",
    "3. uniq\n",
    "4. paste & join\n",
    "5. split\n",
    "6. cut\n",
    "7. grep\n",
    "8. sed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## wc or word count\n",
    "\n",
    "wc is used for obtaining the count of number of words, lines and  characters in a file.\n",
    "\n",
    "\n",
    "1. An example to get number of  lines:\n",
    "\n",
    "```\n",
    "$wc -l <filename>\n",
    "20\n",
    "```\n",
    "\n",
    "2. An example to get number of words:\n",
    "\n",
    "```\n",
    "$wc -w <filename>\n",
    "60\n",
    "```\n",
    "\n",
    "3. An example to get number of characters:\n",
    "```\n",
    "$wc -c <filename>\n",
    "233\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## head:\n",
    "\n",
    "Head is used to ouput the first \"n\" lines from a file. If the number is not specified it outputs the first ten lines. The header can be used to see a sample of the entire data set or it can be used to output partial data set that can be further used for processing. \n",
    "\n",
    "```\n",
    "$head -n <filename>\n",
    "```\n",
    "\n",
    "\n",
    "## sort \n",
    "\n",
    "As the name implies it sorts the lines of text files and prints it out to standard output. You can also sort the file backwords or forwards or based on character position.\n",
    "\n",
    "\n",
    "```\n",
    "$sort [options] <filename>\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## uniq\n",
    "\n",
    "The uniq command is used to remove duplicate lines in from a sorted text file. Or count the occurance of duplicates. \n",
    "\n",
    "> NOTE: The file should be sorted before applying the uniq command. Thats why uniq is always used in combination with the sort command. In unix commands can be pipelined.\n",
    "\n",
    "Suppose you have a file named \"foo.txt\" with the following lines:\n",
    " This line occurs only once.\n",
    " This line occurs twice.\n",
    " This line occurs twice.\n",
    " This line occurs three times.\n",
    " This line occurs three times.\n",
    " This line occurs three times.\n",
    "\n",
    "When you use the uniq command on this file, this is how output looks like:\n",
    "\n",
    "```\n",
    "  $ uniq -c foo.txt\n",
    "    1 This line occurs only once.\n",
    "    2 This line occurs twice.\n",
    "    3 This line occurs three times.\n",
    "```\n",
    "\n",
    "> **NOTE**: The -c option that is used above implies to print the occurance of duplicates as well. The numbers that you see in the output are each the count of the occurance of the line in the entire file. If you dont specify -c it will just output the uniq lines. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## paste & join\n",
    "\n",
    "Paste is used to merge two files as shown below. *items* and *prices* are two text files and have we merge them using paste command:  \n",
    "\n",
    "```\n",
    "$ cat items\n",
    " alphabet blocks\n",
    " building blocks\n",
    " cables\n",
    "\n",
    "$ cat prices\n",
    " $1.00/dozen\n",
    " $2.50 ea.\n",
    " $3.75\n",
    "```\n",
    "\n",
    "The paste Command output:\n",
    "\n",
    "```\n",
    "$ paste items prices\n",
    " alphabet blocks $1.00/dozen\n",
    " building blocks $2.50 ea.\n",
    " cables  $3.75\n",
    "```\n",
    "\n",
    "If you observe the files are merged at line level i.e each line of the first file is merged with the corresponding line in the second file.\n",
    "\n",
    "The join command is an homework for you to explore.\n",
    "\n",
    "\n",
    "\n",
    "## split\n",
    "\n",
    "Imagine you have a very large data file and you want to split this file into equal chunks into different files. Split is the command you are looking for. \n",
    "\n",
    "```\n",
    "  $ split --number=\"< give the number of files or chunks to split here>\"  <filename>\n",
    "```\n",
    "\n",
    "> split can be used in many other ways. You can split based on number of lines. The coolest future is you can split by specifying the size ( in terms of memory such as bytes ). For any case the output is stored in files with prefix  'X' such as xaa, xab etc. You can change the prefix by giving optional parameter.\n",
    "\n",
    "\n",
    "\n",
    "## grep\n",
    "\n",
    "Grep is a utility that searches any number of input files for a given pattern or pattenrs and outputs only those lines which match the pattern. It searches line by line, and if a line matches any one of the patterns it spits it out. What makes grep even more powerfull is you can use basic regular expressions along with simple patters. \n",
    "    \n",
    "#### How do you use grep:\n",
    "\n",
    "\n",
    "1. Lets say you want to search all the occurances of the word  _mining_ in a file named _tutorial.txt_ : \n",
    "\n",
    "   ```\n",
    "     $ grep \"mining\" tutorial.txt\n",
    "   ```\n",
    "   The above command outputs all the lines that have the pattern \"mining\" in the file tutorial.txt. But this includes anything that matches \"mining\" such as \"text-mining\", \"coal-mining\", \"sdfs__mining__sdfd\" and also the mining.\n",
    "\n",
    "2. If you just want the pattern to match whole words then this -w is the option to be used:\n",
    "\n",
    "   ```\n",
    "     $ grep -w \"mining\" tutorial.txt\n",
    "   ```\n",
    "\n",
    "3. The same can be applied for phrases as well . After all a phrase is a set of characters.\n",
    "\n",
    "   ```\n",
    "     $ grep -w \" is a good boy\" tutorial.txt\n",
    "   ```\n",
    "\n",
    "   It will output all the lines that have the phrase \"_is a good boy_\".\n",
    "   \n",
    "\n",
    "#### Regular expressions\n",
    "\n",
    "You can also use regular expressions along with grep. This makes it much more powerfull in terms of pattern search. Its like Tony Stark wearing the iron man suit. \n",
    "\n",
    "\n",
    "```\n",
    "  $ grep '^hello' tutorial.txt\n",
    "```\n",
    "\n",
    "This outputs all those those lines that start with the characters \"_hello_\". If you notice we have used the '^' a metacharacter which represents the start of a line in regular expressions language. For more  on regular expressions please refer  the regular expressions chapter. \n",
    "\n",
    "\n",
    "\n",
    "#### Inverse search:\n",
    "\n",
    "It's not always that you will be searching for lines in a text that match a pattern. There are also instances when  you must search for lines that do not match a pattern. You can do this with grep.\n",
    "\n",
    "```\n",
    "  $ grep -v \"^hello\" tutorial.txt \n",
    "```\n",
    "\n",
    "The option '-v' is to be supplied for this functionaliy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br><br>\n",
    "\n",
    "## Things to try:\n",
    "\n",
    "\n",
    "1. Create a file and save the following text in the file as such:\n",
    "\n",
    "```text\n",
    "\n",
    "Ironman is cool.\n",
    "Ironman is smart\n",
    "Ironman is intelligent.\n",
    "Ironman is good.\n",
    "Ironman is strong.\n",
    "Everyday is a new day\n",
    "Ironman is goood\n",
    "Ironman is smarter\n",
    "```\n",
    "\n",
    "\n",
    "2. Find the number of lines in the file (use wc)\n",
    "\n",
    "3. Check the first line of the file using head. \n",
    "\n",
    "4. Check the lines 1 to 5 using the head file\n",
    "\n",
    "5. sort the lines using Sort command\n",
    "\n",
    "6. Remove duplicates using Uniq command\n",
    "\n",
    "7. Use grep to search for lines that have the word \"smart\" (should not )\n",
    "\n",
    "8. Use grep to get lines that do not have the word \"Ironman\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
